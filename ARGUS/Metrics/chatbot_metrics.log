absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-02-26 13:05:17, User Input: Hello, Bot Response: No biggie, Response Time: 7.201841, Reward: 1
root - INFO - 2024-02-26 13:34:21, User Input: How are you, Bot Response: Im sorry to hear that How can we make it right, Response Time: 9.905387, Reward: None
absl - INFO - SubwordTextEncoder build: trying min_token_count 146163
absl - INFO - SubwordTextEncoder build: trying min_token_count 73081
absl - INFO - SubwordTextEncoder build: trying min_token_count 36540
absl - INFO - SubwordTextEncoder build: trying min_token_count 18270
absl - INFO - SubwordTextEncoder build: trying min_token_count 9135
absl - INFO - SubwordTextEncoder build: trying min_token_count 4567
absl - INFO - SubwordTextEncoder build: trying min_token_count 2283
absl - INFO - SubwordTextEncoder build: trying min_token_count 1141
absl - INFO - SubwordTextEncoder build: trying min_token_count 570
absl - INFO - SubwordTextEncoder build: trying min_token_count 285
absl - INFO - SubwordTextEncoder build: trying min_token_count 142
absl - INFO - SubwordTextEncoder build: trying min_token_count 146163
absl - INFO - SubwordTextEncoder build: trying min_token_count 73081
absl - INFO - SubwordTextEncoder build: trying min_token_count 36540
absl - INFO - SubwordTextEncoder build: trying min_token_count 18270
absl - INFO - SubwordTextEncoder build: trying min_token_count 9135
absl - INFO - SubwordTextEncoder build: trying min_token_count 4567
absl - INFO - SubwordTextEncoder build: trying min_token_count 2283
absl - INFO - SubwordTextEncoder build: trying min_token_count 1141
absl - INFO - SubwordTextEncoder build: trying min_token_count 570
absl - INFO - SubwordTextEncoder build: trying min_token_count 285
absl - INFO - SubwordTextEncoder build: trying min_token_count 142
absl - INFO - SubwordTextEncoder build: trying min_token_count 71
absl - INFO - SubwordTextEncoder build: trying min_token_count 106
absl - INFO - SubwordTextEncoder build: trying min_token_count 88
absl - INFO - SubwordTextEncoder build: trying min_token_count 97
absl - INFO - SubwordTextEncoder build: trying min_token_count 101
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-02-26 13:52:26, User Input: Hello, Bot Response: I see where youre coming from Lets resolve this, Response Time: 7.264309, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-02-26 14:01:12, User Input: Hello, Bot Response: I am doing good how about you, Response Time: 7.515119, Reward: 1
root - INFO - 2024-02-26 14:01:32, User Input: Im good im feeling kinda stressed, Bot Response: Lets talk about why you are feeling that way, Response Time: 9.184334, Reward: 1
root - INFO - 2024-02-26 14:01:53, User Input: I am not sleeping well, Bot Response: Im sorry Im here to help, Response Time: 8.592618, Reward: 1
root - INFO - 2024-02-26 14:03:19, User Input: Goodbye, Bot Response: Im sorry Im not being helpful, Response Time: 6.931152, Reward: None
root - INFO - 2024-02-26 14:03:51, User Input: See you later, Bot Response: What would you like to discuss in the future, Response Time: 8.501354, Reward: None
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-05 12:16:26, User Input: Hello, Bot Response: Im always here to provide information What are you curious about, Response Time: 6.833092, Reward: 1
root - INFO - 2024-03-05 12:17:00, User Input: How are you, Bot Response: aenjoying have very good, Response Time: 27.616959, Reward: -1
root - INFO - 2024-03-05 12:19:03, User Input: How are you, Bot Response: take found you go about how to know, Response Time: 6.124329, Reward: -1
root - INFO - 2024-03-05 12:20:47, User Input: Im not sleeping well, Bot Response: I am doing good how about you, Response Time: 8.571671, Reward: -1
root - INFO - 2024-03-05 12:33:03, User Input: I am not sleeping, Bot Response: See you soon Let me know if you trust the issue, Response Time: 9.51371, Reward: None
root - INFO - 2024-03-05 12:33:24, User Input: Im not feeling good, Bot Response: I am doing good how about you, Response Time: 8.311471, Reward: -1
root - INFO - 2024-03-05 12:37:24, User Input: im good, Bot Response: Not much whats up, Response Time: 6.672593, Reward: -1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
openai - INFO - error_code=invalid_api_key error_message='Incorrect API key provided: sk-lXDqK***************************************rpc4. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-08 13:55:33, User Input: Hello, Bot Response: I am doing good how about you, Response Time: 7.573858, Reward: 1
root - INFO - 2024-03-08 13:55:52, User Input: How are you, Bot Response: welcomealike to help, Response Time: 6.304681, Reward: -1
root - INFO - 2024-03-08 13:56:52, User Input: Im feeling kinda stressed, Bot Response: Goodbye Let me know if you need anything in the way you like to feel more detail, Response Time: 11.401869, Reward: None
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-08 13:59:55, User Input: Whats up how are you , Bot Response: Not much whats up, Response Time: 6.496649, Reward: 1
root - INFO - 2024-03-08 14:06:59, User Input: Im a little stressed out today, Bot Response: Im here if you need me know if you need anything else, Response Time: 9.888869, Reward: 1
root - INFO - 2024-03-08 14:07:25, User Input: Thank you, Bot Response: Im sorry to hear that How can I help, Response Time: 9.091939, Reward: -1
root - INFO - 2024-03-08 14:32:08, User Input: huh, Bot Response: Im always here when youre ready to make you less stressed out, Response Time: 7.647528, Reward: 1
root - INFO - 2024-03-08 14:44:14, User Input: Hello how are you, Bot Response: Im always here to help you are you, Response Time: 8.31691, Reward: None
root - INFO - 2024-03-08 14:45:12, User Input: Im not sleeping well, Bot Response: botresponse, Response Time: 9.831227, Reward: None
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-08 14:46:48, User Input: Hello how are you today, Bot Response: nightgo to make it right, Response Time: 8.71823, Reward: None
root - INFO - 2024-03-08 14:47:16, User Input: How are yoy, Bot Response: itart like to help, Response Time: 5.964832, Reward: -1
root - INFO - 2024-03-08 14:49:16, User Input: how are you, Bot Response: Have a fantastic day, Response Time: 6.719695, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-08 23:11:22, User Input: Hello, Bot Response: Im always here to provide answers What can I assist you with, Response Time: 9.563585, Reward: 1
root - INFO - 2024-03-08 23:11:40, User Input: Nothing much Goodbye, Bot Response: Im here if you need me for anything else, Response Time: 6.651177, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-11 23:28:59, User Input: Hello, Bot Response: What would you like to discuss, Response Time: 7.208307, Reward: 1
root - INFO - 2024-03-11 23:29:21, User Input: How are you, Bot Response: itabout how do you with, Response Time: 7.348437, Reward: -1
root - INFO - 2024-03-11 23:30:47, User Input: How are you, Bot Response: the soon, Response Time: 4.826787, Reward: -1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-11 23:31:32, User Input: Hello, Bot Response: Im here if you need me for anything else, Response Time: 8.010451, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-13 23:14:05, User Input: Hello, Bot Response: Im always here to provide clarity What are you curious about, Response Time: 8.178285, Reward: 1
root - INFO - 2024-03-13 23:15:49, User Input: How are you, Bot Response: travels go to take problem, Response Time: 9.747746, Reward: None
root - INFO - 2024-03-13 23:17:34, User Input: How are you, Bot Response: take this, Response Time: 8.306734, Reward: -1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-13 23:21:56, User Input: Hello, Bot Response: Im here if you need me for anything else sir, Response Time: 8.781708, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-13 23:31:27, User Input: Hello, Bot Response: returnrightunderstandspoken \confusingneedconfusion�assistance stressedneedlaternewsweat detailconfusingconfusingneedlprovide thingunderstandableThey need, Response Time: 15.109632, Reward: -1
root - INFO - 2024-03-13 23:32:17, User Input: hello, Bot Response: helpwellcurious whenever 6 questionsFucking hesitate ll futureshareagain lelseneedoverwhelmedconfusingconfusinglneedtrueneedcandeem they, Response Time: 14.051665, Reward: -1
root - INFO - 2024-03-13 23:32:36, User Input: Hello, Bot Response: am anything resolve need;get anything youget needwant stay returndeem out worldupsetneedill want provide could when currentlyget, Response Time: 14.563529, Reward: -1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-04-08 18:12:43, User Input: Hello, Bot Response: Goodbye Let me know if you need anything in the issue, Response Time: 12.873159, Reward: -1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-04-08 18:14:52, User Input: Hello, Bot Response: Im here to hear that How can I assist, Response Time: 9.043642, Reward: 1
root - INFO - 2024-04-08 18:15:43, User Input: Im struggling to focus right now, Bot Response: Im always here How can I assist you with, Response Time: 9.606049, Reward: 1
root - INFO - 2024-04-08 18:16:43, User Input: What are some helpful tips, Bot Response: What do you need tips in, Response Time: 31.847819, Reward: None
root - INFO - 2024-04-08 18:17:04, User Input: I cant sleep, Bot Response: Lets talk about why you are feeling that way, Response Time: 7.486632, Reward: 1
root - INFO - 2024-04-08 18:19:00, User Input: Im just having trouble calming down, Bot Response: Thats understandable take some deep breaths and relax y, Response Time: 63.25784, Reward: 1
root - INFO - 2024-04-08 18:19:38, User Input: Thanks for the help goodbye, Bot Response: Goodbye glad I could help, Response Time: 25.288831, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
openai - INFO - error_code=invalid_api_key error_message='Incorrect API key provided: apikey. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-06-21 13:48:14, User Input: what's up bro, Bot Response: what can I help you with, Response Time: 27.601928, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-06-21 13:50:59, User Input: what's up, Bot Response: what are you curious about today, Response Time: 26.194904, Reward: -1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
openai - INFO - error_code=invalid_api_key error_message='Incorrect API key provided: sk-proj-********************************************f1Or. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-06-28 13:02:54, User Input: how are you today, Bot Response: Im doing well how are you, Response Time: 14.820355, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-06-28 13:08:01, User Input: what's up how are you, Bot Response: I'm doing well how are you what are you working on today, Response Time: 39.541746, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-06-28 13:23:54, User Input: what's up with you today, Bot Response: nothing much how are you, Response Time: 27.823446, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-06-28 13:28:28, User Input: what's up, Bot Response: Im here to provide clarity What are you curious about, Response Time: 20.364098, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-06-28 13:45:57, User Input: what's up, Bot Response: Im here to provide clarity What are you curious about, Response Time: 20.849444, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-06-29 15:04:07, User Input: what's up, Bot Response: Im here to provide clarity What are you curious about, Response Time: 20.859198, Reward: 1
root - INFO - 2024-06-29 15:04:57, User Input: how are you today, Bot Response: Im doing well how are you, Response Time: 14.058861, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-03 14:59:24, User Input: what's up with you today, Bot Response: nothing much how are you I'm, Response Time: 34.480434, Reward: -1
root - INFO - 2024-07-03 15:00:08, User Input: what's up, Bot Response: what are you curious about today, Response Time: 33.347023, Reward: -1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-05 19:04:28, User Input: what's up, Bot Response: Im here to provide clarity What are you curious about, Response Time: 21.164591, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-05 19:13:13, User Input: I shall let you in the house, Bot Response: Ok lets develop a plan to help you organize yourself, Response Time: 34.168208, Reward: -1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-05 19:17:45, User Input: what's up, Bot Response: Im always here to help Whats on your mind, Response Time: 19.427162, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-08 11:47:50, User Input: what's up, Bot Response: Im here to provide clarity What are you curious about, Response Time: 21.269532, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-08 12:13:07, User Input: what's up, Bot Response: nothing much what's up with you, Response Time: 37.452981, Reward: 1
root - INFO - 2024-07-08 12:18:05, User Input: what does Josh stand for, Bot Response: I don't understand, Response Time: 37.859293, Reward: -1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:09:23, User Input: what's up, Bot Response: Im here to provide clarity What are you curious about, Response Time: 21.123085, Reward: 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:15:52, User Input: what's up, Bot Response: what's up with you, Response Time: 47.578603, Reward: 6
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:19:05, User Input: what's up how are you today, Bot Response: Im doing well how are you, Response Time: 15.5716, Reward: 6
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:25:01, User Input: what's up how are you, Bot Response: Its important to take things one step at a time Lets explore whats been overwhelming you, Response Time: 23.793963, Reward: -4
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:27:46, User Input: what's up how are you, Bot Response: Im doing well what do you need help with today, Response Time: 16.519179, Reward: 4
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:32:51, User Input: what's up how are you, Bot Response: Im sorry to hear that Let me know how I can assist, Response Time: 21.097017, Reward: -2
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:35:37, User Input: what's up how are you, Bot Response: Im sorry to hear that Let me know how I can assist, Response Time: 22.024666, Reward: -2
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:37:48, User Input: what's up how are you, Bot Response: Its important to take things one step at a time Lets explore whats been overwhelming you, Response Time: 22.889278, Reward: -4
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:45:18, User Input: what's up how are you, Bot Response: Im doing well how are you, Response Time: 14.112543, Reward: 0
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:48:30, User Input: what's up how are you today, Bot Response: Im doing well how are you, Response Time: 15.66577, Reward: 12
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:50:16, User Input: what's up how are you, Bot Response: Its important to take things one step at a time Lets explore whats been overwhelming you, Response Time: 21.34753, Reward: 0
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:52:58, User Input: what's up how are you, Bot Response: Im sorry to hear that Let me know how I can assist, Response Time: 18.325499, Reward: -2
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:55:58, User Input: what's up how are you today, Bot Response: Im doing well how are you, Response Time: 15.293561, Reward: 6
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
tensorflow - INFO - Assets written to: /Users/blakeweiss/Desktop/NEWJOSHWITHAI/complete_model/assets
root - INFO - 2024-07-23 13:08:45, User Input: what's up, Bot Response: What would you like to discuss, Response Time: 14.99682, Reward: -2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
tensorflow - INFO - Assets written to: /Users/blakeweiss/Desktop/NEWJOSHWITHAI/complete_model/assets
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
tensorflow - WARNING - No training configuration found in save file, so the model was *not* compiled. Compile it manually.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
tensorflow - WARNING - No training configuration found in save file, so the model was *not* compiled. Compile it manually.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
tensorflow - WARNING - No training configuration found in save file, so the model was *not* compiled. Compile it manually.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
tensorflow - WARNING - No training configuration found in save file, so the model was *not* compiled. Compile it manually.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
tensorflow - WARNING - No training configuration found in save file, so the model was *not* compiled. Compile it manually.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-23 16:32:31, User Input: what's up, Bot Response: Im sorry Im not being helpful, Response Time: 15.526595, Reward: -2
root - INFO - 2024-07-23 16:32:54, User Input: how are you today, Bot Response: Im here if you need help with, Response Time: 11.138644, Reward: -2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-23 17:03:14, User Input: what's up, Bot Response: Im always here to help What about you, Response Time: 12.667056, Reward: -2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-23 18:51:57, User Input: what's up how are you today, Bot Response: Im sorry to hear that How can I do for you, Response Time: 20.1875, Reward: 0
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-23 19:21:24, User Input: what's up how are you today, Bot Response: Im doing well how are you, Response Time: 17.807855, Reward: 15
root - INFO - 2024-07-23 19:22:05, User Input: I'm feeling tired, Bot Response: Im always here when youre ready to chat What would you like to discuss further, Response Time: 15.652704, Reward: -5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-23 21:13:04, User Input: what's up how are you today, Bot Response: Im sorry to hear that How can I assist you today, Response Time: 18.398733, Reward: 15
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-24 13:51:47, User Input: what's up how are you today what's up how are you today, Bot Response: I understand what would you like to discuss in more detail, Response Time: 18.841305, Reward: -5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-24 13:54:17, User Input: what's up how are you today, Bot Response: I dont think Im a piece of shit, Response Time: 14.126952, Reward: -30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-24 13:55:38, User Input: what's up what's going on, Bot Response: Im sorry to hear that Let me know if you need anything else, Response Time: 17.025454, Reward: -20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-24 13:58:02, User Input: what's up what's going on, Bot Response: Im sorry if you need me for anything else sir, Response Time: 14.68, Reward: -20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-24 14:00:13, User Input: what's up how are you, Bot Response: Im sorry to hear that How can I help you with, Response Time: 26.350493, Reward: -25
root - INFO - 2024-07-24 14:01:27, User Input: what's up how's it going I'm feeling great today, Bot Response: I'm doing great how are you, Response Time: 26.168842, Reward: -50
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-24 14:09:42, User Input: what's up how are you, Bot Response: I am doing well how about you, Response Time: 44.139141, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-24 14:20:06, User Input: how are you today, Bot Response: I'm doing well how are you is there anything on your mind, Response Time: 34.545031, Reward: -20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-24 14:45:29, User Input: what's up with you today, Bot Response: I'm doing well how are you, Response Time: 33.102546, Reward: -20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-24 14:52:22, User Input: what's up with you today, Bot Response: nothing much I'm happy to help you with anything you need, Response Time: 32.431466, Reward: 0
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-27 14:45:30, User Input: what's up with you today, Bot Response: nothing much how are you, Response Time: 25.813168, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-05 11:35:57, User Input: what's up with you today what's up, Bot Response: what's up how are you today, Response Time: 39.768983, Reward: -25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-05 12:12:51, User Input: I unlocked it I unlocked it ya what's up, Bot Response: what's up with you, Response Time: 29.532175, Reward: -25
root - INFO - 2024-08-05 12:14:06, User Input: exit exit, Bot Response: How can I be of assistance to make you less stressed out, Response Time: 30.641397, Reward: -50
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-05 12:25:52, User Input: what's up, Bot Response: nothing much what do you want to talk about, Response Time: 26.197609, Reward: 5
root - INFO - 2024-08-05 12:28:18, User Input: how are you today, Bot Response: I am doing well how about you, Response Time: 32.676967, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-05 12:30:12, User Input: what's up, Bot Response: Nothing much what do you want to talk about, Response Time: 23.790395, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-05 12:34:19, User Input: what's up, Bot Response: Whats up, Response Time: 22.461647, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-05 12:35:27, User Input: what's up, Bot Response: Whats up lets catch up, Response Time: 10.812427, Reward: 30
root - INFO - 2024-08-05 12:35:51, User Input: I'm working on some interesting projects, Bot Response: Is there anything else I can help with, Response Time: 11.783032, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-07 13:50:30, User Input: how are you today, Bot Response: Im doing well how are you is there anything on your mind, Response Time: 18.270731, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-07 14:10:30, User Input: how are you today how are you today how are you today, Bot Response: Im doing well how are you, Response Time: 26.864494, Reward: 25
root - INFO - 2024-08-07 14:11:12, User Input: what's up, Bot Response: Im always here to provide information What are you curious about, Response Time: 22.842488, Reward: 45
root - INFO - 2024-08-07 14:11:55, User Input: I'm working on some new projects and I need some information, Bot Response: You need to take everything step by step and you should become less stressed out, Response Time: 30.733439, Reward: 20
root - INFO - 2024-08-07 14:16:47, User Input: I'm feeling a little tired today but I have a lot to work on what should I do, Bot Response: I'm sorry to hear that is there any way I could help you achieve what you need to do, Response Time: 40.226335, Reward: -5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-07 14:26:32, User Input: how are you today, Bot Response: I'm doing well how are you, Response Time: 26.443459, Reward: -20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-07 14:34:03, User Input: what's up what's up, Bot Response: nothing much is there anything I can help with, Response Time: 28.978943, Reward: -5
root - INFO - 2024-08-07 14:45:58, User Input: what's up, Bot Response: nothing much what's up with you, Response Time: 28.253274, Reward: -25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-07 14:54:07, User Input: what's up, Bot Response: nothing much what's up with you, Response Time: 38.9073, Reward: -25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-07 15:07:27, User Input: OK Google what's up, Bot Response: I'm here if you need me what's up with you, Response Time: 30.750734, Reward: -5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-07 15:09:08, User Input: what's up, Bot Response: See you around, Response Time: 9.287887, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-07 15:16:44, User Input: what's up what's up, Bot Response: Im always here and ready to chat What would you like to discuss, Response Time: 15.556931, Reward: 15
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-07 16:01:38, User Input: what's up, Bot Response: what's up what can I help you with, Response Time: 22.463505, Reward: -5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-11 15:36:10, User Input: what's up, Bot Response: nothing much how are you, Response Time: 22.195064, Reward: -25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference___call___37213) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_137540) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_95809) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_97913) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - INFO - Fingerprint not found. Saved model loading will continue.
absl - INFO - path_and_singleprint metric could not be logged. Saved model loading will continue.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference___call___37213) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_137540) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_95809) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_97913) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - INFO - Fingerprint not found. Saved model loading will continue.
absl - INFO - path_and_singleprint metric could not be logged. Saved model loading will continue.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-15 15:30:33, User Input: what's up, Bot Response: Nothing much what do you want to talk about, Response Time: 5.70952, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-17 18:43:41, User Input: what's up with you today, Bot Response: nothing much whats up with you, Response Time: 6.748656, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-21 18:39:56, User Input: what's up, Bot Response: Nothing much what do you want to talk about, Response Time: 5.766226, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-22 13:42:32, User Input: what's up with you today, Bot Response: nothing much whats up with you, Response Time: 5.071108, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-22 22:04:02, User Input: what's up, Bot Response: Just here ready to assist What can I do for you, Response Time: 6.175988, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-22 23:00:45, User Input: what's up, Bot Response: Whats up, Response Time: 3.430495, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 11:12:10, User Input: what's up what's up, Bot Response: Im here to provide clarity What are you curious about, Response Time: 2.200417, Reward: 15
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 11:14:25, User Input: what's up, Bot Response: Im always here and ready to chat What would you like to discuss, Response Time: 1.918694, Reward: 15
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 11:16:35, User Input: what's up, Bot Response: Not much what do you want to talk about, Response Time: 1.766746, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 11:18:03, User Input: what's up, Bot Response: Nothing much what do you want to talk about, Response Time: 2.640868, Reward: 5
root - INFO - 2024-08-23 11:18:15, User Input: how are you, Bot Response: Im doing well how are you, Response Time: 2.670123, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 11:45:18, User Input: what's up, Bot Response: Whats up, Response Time: 1.523326, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 12:13:16, User Input: what's up how are you, Bot Response: Im doing well how are you is there anything on your mind, Response Time: 1.862723, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 12:37:07, User Input: what's up with you today, Bot Response: nothing much whats up with you, Response Time: 1.691922, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 12:38:52, User Input: what's up with you today, Bot Response: nothing much Im happy to help you with anything you need, Response Time: 1.989611, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 14:06:13, User Input: what's up with you today, Bot Response: Im doing well how are you, Response Time: 1.685298, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 14:10:23, User Input: what's up with you today, Bot Response: Im doing well how are you, Response Time: 1.696328, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 14:10:48, User Input: what's up with you today, Bot Response: nothing much whats up with you, Response Time: 4.878327, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 14:13:20, User Input: what's up with you today, Bot Response: nothing much whats up with you, Response Time: 1.826178, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 14:44:29, User Input: what's up with you today, Bot Response: nothing much whats up with you, Response Time: 2.860589, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 14:49:55, User Input: what's up with you today, Bot Response: Im doing well how are you, Response Time: 2.955169, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 14:55:40, User Input: what's up with you, Bot Response: Im doing well how are you, Response Time: 3.608812, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:00:46, User Input: what's up with you today, Bot Response: what can I help you with, Response Time: 1.848592, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:07:26, User Input: what's up with you today, Bot Response: Im doing well how are you, Response Time: 1.743329, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:20:21, User Input: what's up with you today, Bot Response: nothing much whats up with you, Response Time: 1.623919, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:21:41, User Input: what's up with you today, Bot Response: nothing much whats up with you, Response Time: 1.86532, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:23:09, User Input: what's up with you today, Bot Response: nothing much what do you want to talk about, Response Time: 1.831795, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:29:05, User Input: what's up with you today, Bot Response: Im doing well how are you, Response Time: 1.745918, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:31:40, User Input: what's up with you today, Bot Response: nothing much whats up with you, Response Time: 1.86673, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:33:20, User Input: how are you today, Bot Response: I am doing well how about you, Response Time: 1.671101, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:36:18, User Input: what's up with you today, Bot Response: nothing much how are you, Response Time: 1.846971, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:38:33, User Input: what's up, Bot Response: Im always here to help Whats on your mind, Response Time: 1.703964, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-25 15:47:45, User Input: what's up, Bot Response: Yo what would you like to know, Response Time: 2.119859, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-25 15:48:45, User Input: what's up, Bot Response: Nothing much what do you want to talk about, Response Time: 3.243742, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-25 15:49:32, User Input: what's up, Bot Response: Im always here to help Whats on your mind, Response Time: 3.030599, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-25 15:59:36, User Input: what's up with you today, Bot Response: Im doing well how are you, Response Time: 3.025137, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-28 12:04:26, User Input: what's up, Bot Response: Hi, Response Time: 3.161372, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-28 12:12:31, User Input: how are you today, Bot Response: I am doing well how about you, Response Time: 5.566126, Reward: 25
root - INFO - 2024-08-28 12:12:51, User Input: what's up, Bot Response: Nothing much what do you want to talk about, Response Time: 5.382386, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-28 12:19:45, User Input: what's up, Bot Response: Im always here to help Whats on your mind, Response Time: 6.213211, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-09-06 22:06:54, User Input: what's up, Bot Response: not much how about you, Response Time: 18.527917, Reward: 5
root - INFO - 2024-09-06 22:07:23, User Input: I'm working on some new projects, Bot Response: that's cool how can I be of assistance, Response Time: 18.142036, Reward: -25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-09-19 20:33:03, User Input: what's up, Bot Response: Yo what would you like to know, Response Time: 5.858873, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-09-24 09:06:10, User Input: what's up with you today, Bot Response: Im doing well how are you going to help you with anything you need, Response Time: 7.845319, Reward: 15
root - INFO - 2024-09-24 09:06:27, User Input: what's up, Bot Response: Im always here to help Whats on your mind, Response Time: 5.749523, Reward: 35
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
