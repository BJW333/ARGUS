absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-02-26 13:05:17, User Input: Hello, Bot Response: No biggie, Response Time: 7.201841, Reward: 1
root - INFO - 2024-02-26 13:34:21, User Input: How are you, Bot Response: Im sorry to hear that How can we make it right, Response Time: 9.905387, Reward: None
absl - INFO - SubwordTextEncoder build: trying min_token_count 146163
absl - INFO - SubwordTextEncoder build: trying min_token_count 73081
absl - INFO - SubwordTextEncoder build: trying min_token_count 36540
absl - INFO - SubwordTextEncoder build: trying min_token_count 18270
absl - INFO - SubwordTextEncoder build: trying min_token_count 9135
absl - INFO - SubwordTextEncoder build: trying min_token_count 4567
absl - INFO - SubwordTextEncoder build: trying min_token_count 2283
absl - INFO - SubwordTextEncoder build: trying min_token_count 1141
absl - INFO - SubwordTextEncoder build: trying min_token_count 570
absl - INFO - SubwordTextEncoder build: trying min_token_count 285
absl - INFO - SubwordTextEncoder build: trying min_token_count 142
absl - INFO - SubwordTextEncoder build: trying min_token_count 146163
absl - INFO - SubwordTextEncoder build: trying min_token_count 73081
absl - INFO - SubwordTextEncoder build: trying min_token_count 36540
absl - INFO - SubwordTextEncoder build: trying min_token_count 18270
absl - INFO - SubwordTextEncoder build: trying min_token_count 9135
absl - INFO - SubwordTextEncoder build: trying min_token_count 4567
absl - INFO - SubwordTextEncoder build: trying min_token_count 2283
absl - INFO - SubwordTextEncoder build: trying min_token_count 1141
absl - INFO - SubwordTextEncoder build: trying min_token_count 570
absl - INFO - SubwordTextEncoder build: trying min_token_count 285
absl - INFO - SubwordTextEncoder build: trying min_token_count 142
absl - INFO - SubwordTextEncoder build: trying min_token_count 71
absl - INFO - SubwordTextEncoder build: trying min_token_count 106
absl - INFO - SubwordTextEncoder build: trying min_token_count 88
absl - INFO - SubwordTextEncoder build: trying min_token_count 97
absl - INFO - SubwordTextEncoder build: trying min_token_count 101
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-02-26 13:52:26, User Input: Hello, Bot Response: I see where youre coming from Lets resolve this, Response Time: 7.264309, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-02-26 14:01:12, User Input: Hello, Bot Response: I am doing good how about you, Response Time: 7.515119, Reward: 1
root - INFO - 2024-02-26 14:01:32, User Input: Im good im feeling kinda stressed, Bot Response: Lets talk about why you are feeling that way, Response Time: 9.184334, Reward: 1
root - INFO - 2024-02-26 14:01:53, User Input: I am not sleeping well, Bot Response: Im sorry Im here to help, Response Time: 8.592618, Reward: 1
root - INFO - 2024-02-26 14:03:19, User Input: Goodbye, Bot Response: Im sorry Im not being helpful, Response Time: 6.931152, Reward: None
root - INFO - 2024-02-26 14:03:51, User Input: See you later, Bot Response: What would you like to discuss in the future, Response Time: 8.501354, Reward: None
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-05 12:16:26, User Input: Hello, Bot Response: Im always here to provide information What are you curious about, Response Time: 6.833092, Reward: 1
root - INFO - 2024-03-05 12:17:00, User Input: How are you, Bot Response: aenjoying have very good, Response Time: 27.616959, Reward: -1
root - INFO - 2024-03-05 12:19:03, User Input: How are you, Bot Response: take found you go about how to know, Response Time: 6.124329, Reward: -1
root - INFO - 2024-03-05 12:20:47, User Input: Im not sleeping well, Bot Response: I am doing good how about you, Response Time: 8.571671, Reward: -1
root - INFO - 2024-03-05 12:33:03, User Input: I am not sleeping, Bot Response: See you soon Let me know if you trust the issue, Response Time: 9.51371, Reward: None
root - INFO - 2024-03-05 12:33:24, User Input: Im not feeling good, Bot Response: I am doing good how about you, Response Time: 8.311471, Reward: -1
root - INFO - 2024-03-05 12:37:24, User Input: im good, Bot Response: Not much whats up, Response Time: 6.672593, Reward: -1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
openai - INFO - error_code=invalid_api_key error_message='Incorrect API key provided: sk-lXDqK***************************************rpc4. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-08 13:55:33, User Input: Hello, Bot Response: I am doing good how about you, Response Time: 7.573858, Reward: 1
root - INFO - 2024-03-08 13:55:52, User Input: How are you, Bot Response: welcomealike to help, Response Time: 6.304681, Reward: -1
root - INFO - 2024-03-08 13:56:52, User Input: Im feeling kinda stressed, Bot Response: Goodbye Let me know if you need anything in the way you like to feel more detail, Response Time: 11.401869, Reward: None
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-08 13:59:55, User Input: Whats up how are you , Bot Response: Not much whats up, Response Time: 6.496649, Reward: 1
root - INFO - 2024-03-08 14:06:59, User Input: Im a little stressed out today, Bot Response: Im here if you need me know if you need anything else, Response Time: 9.888869, Reward: 1
root - INFO - 2024-03-08 14:07:25, User Input: Thank you, Bot Response: Im sorry to hear that How can I help, Response Time: 9.091939, Reward: -1
root - INFO - 2024-03-08 14:32:08, User Input: huh, Bot Response: Im always here when youre ready to make you less stressed out, Response Time: 7.647528, Reward: 1
root - INFO - 2024-03-08 14:44:14, User Input: Hello how are you, Bot Response: Im always here to help you are you, Response Time: 8.31691, Reward: None
root - INFO - 2024-03-08 14:45:12, User Input: Im not sleeping well, Bot Response: botresponse, Response Time: 9.831227, Reward: None
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-08 14:46:48, User Input: Hello how are you today, Bot Response: nightgo to make it right, Response Time: 8.71823, Reward: None
root - INFO - 2024-03-08 14:47:16, User Input: How are yoy, Bot Response: itart like to help, Response Time: 5.964832, Reward: -1
root - INFO - 2024-03-08 14:49:16, User Input: how are you, Bot Response: Have a fantastic day, Response Time: 6.719695, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-08 23:11:22, User Input: Hello, Bot Response: Im always here to provide answers What can I assist you with, Response Time: 9.563585, Reward: 1
root - INFO - 2024-03-08 23:11:40, User Input: Nothing much Goodbye, Bot Response: Im here if you need me for anything else, Response Time: 6.651177, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-11 23:28:59, User Input: Hello, Bot Response: What would you like to discuss, Response Time: 7.208307, Reward: 1
root - INFO - 2024-03-11 23:29:21, User Input: How are you, Bot Response: itabout how do you with, Response Time: 7.348437, Reward: -1
root - INFO - 2024-03-11 23:30:47, User Input: How are you, Bot Response: the soon, Response Time: 4.826787, Reward: -1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-11 23:31:32, User Input: Hello, Bot Response: Im here if you need me for anything else, Response Time: 8.010451, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-13 23:14:05, User Input: Hello, Bot Response: Im always here to provide clarity What are you curious about, Response Time: 8.178285, Reward: 1
root - INFO - 2024-03-13 23:15:49, User Input: How are you, Bot Response: travels go to take problem, Response Time: 9.747746, Reward: None
root - INFO - 2024-03-13 23:17:34, User Input: How are you, Bot Response: take this, Response Time: 8.306734, Reward: -1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-13 23:21:56, User Input: Hello, Bot Response: Im here if you need me for anything else sir, Response Time: 8.781708, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 328
absl - INFO - SubwordTextEncoder build: trying min_token_count 164
absl - INFO - SubwordTextEncoder build: trying min_token_count 82
absl - INFO - SubwordTextEncoder build: trying min_token_count 41
absl - INFO - SubwordTextEncoder build: trying min_token_count 20
absl - INFO - SubwordTextEncoder build: trying min_token_count 10
absl - INFO - SubwordTextEncoder build: trying min_token_count 5
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-03-13 23:31:27, User Input: Hello, Bot Response: returnrightunderstandspoken \confusingneedconfusion�assistance stressedneedlaternewsweat detailconfusingconfusingneedlprovide thingunderstandableThey need, Response Time: 15.109632, Reward: -1
root - INFO - 2024-03-13 23:32:17, User Input: hello, Bot Response: helpwellcurious whenever 6 questionsFucking hesitate ll futureshareagain lelseneedoverwhelmedconfusingconfusinglneedtrueneedcandeem they, Response Time: 14.051665, Reward: -1
root - INFO - 2024-03-13 23:32:36, User Input: Hello, Bot Response: am anything resolve need;get anything youget needwant stay returndeem out worldupsetneedill want provide could when currentlyget, Response Time: 14.563529, Reward: -1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-04-08 18:12:43, User Input: Hello, Bot Response: Goodbye Let me know if you need anything in the issue, Response Time: 12.873159, Reward: -1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-04-08 18:14:52, User Input: Hello, Bot Response: Im here to hear that How can I assist, Response Time: 9.043642, Reward: 1
root - INFO - 2024-04-08 18:15:43, User Input: Im struggling to focus right now, Bot Response: Im always here How can I assist you with, Response Time: 9.606049, Reward: 1
root - INFO - 2024-04-08 18:16:43, User Input: What are some helpful tips, Bot Response: What do you need tips in, Response Time: 31.847819, Reward: None
root - INFO - 2024-04-08 18:17:04, User Input: I cant sleep, Bot Response: Lets talk about why you are feeling that way, Response Time: 7.486632, Reward: 1
root - INFO - 2024-04-08 18:19:00, User Input: Im just having trouble calming down, Bot Response: Thats understandable take some deep breaths and relax y, Response Time: 63.25784, Reward: 1
root - INFO - 2024-04-08 18:19:38, User Input: Thanks for the help goodbye, Bot Response: Goodbye glad I could help, Response Time: 25.288831, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
openai - INFO - error_code=invalid_api_key error_message='Incorrect API key provided: apikey. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-06-21 13:48:14, User Input: what's up bro, Bot Response: what can I help you with, Response Time: 27.601928, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-06-21 13:50:59, User Input: what's up, Bot Response: what are you curious about today, Response Time: 26.194904, Reward: -1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
openai - INFO - error_code=invalid_api_key error_message='Incorrect API key provided: sk-proj-********************************************f1Or. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-06-28 13:02:54, User Input: how are you today, Bot Response: Im doing well how are you, Response Time: 14.820355, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-06-28 13:08:01, User Input: what's up how are you, Bot Response: I'm doing well how are you what are you working on today, Response Time: 39.541746, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-06-28 13:23:54, User Input: what's up with you today, Bot Response: nothing much how are you, Response Time: 27.823446, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-06-28 13:28:28, User Input: what's up, Bot Response: Im here to provide clarity What are you curious about, Response Time: 20.364098, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-06-28 13:45:57, User Input: what's up, Bot Response: Im here to provide clarity What are you curious about, Response Time: 20.849444, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-06-29 15:04:07, User Input: what's up, Bot Response: Im here to provide clarity What are you curious about, Response Time: 20.859198, Reward: 1
root - INFO - 2024-06-29 15:04:57, User Input: how are you today, Bot Response: Im doing well how are you, Response Time: 14.058861, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-03 14:59:24, User Input: what's up with you today, Bot Response: nothing much how are you I'm, Response Time: 34.480434, Reward: -1
root - INFO - 2024-07-03 15:00:08, User Input: what's up, Bot Response: what are you curious about today, Response Time: 33.347023, Reward: -1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-05 19:04:28, User Input: what's up, Bot Response: Im here to provide clarity What are you curious about, Response Time: 21.164591, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-05 19:13:13, User Input: I shall let you in the house, Bot Response: Ok lets develop a plan to help you organize yourself, Response Time: 34.168208, Reward: -1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-05 19:17:45, User Input: what's up, Bot Response: Im always here to help Whats on your mind, Response Time: 19.427162, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-08 11:47:50, User Input: what's up, Bot Response: Im here to provide clarity What are you curious about, Response Time: 21.269532, Reward: 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-08 12:13:07, User Input: what's up, Bot Response: nothing much what's up with you, Response Time: 37.452981, Reward: 1
root - INFO - 2024-07-08 12:18:05, User Input: what does Josh stand for, Bot Response: I don't understand, Response Time: 37.859293, Reward: -1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:09:23, User Input: what's up, Bot Response: Im here to provide clarity What are you curious about, Response Time: 21.123085, Reward: 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:15:52, User Input: what's up, Bot Response: what's up with you, Response Time: 47.578603, Reward: 6
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:19:05, User Input: what's up how are you today, Bot Response: Im doing well how are you, Response Time: 15.5716, Reward: 6
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:25:01, User Input: what's up how are you, Bot Response: Its important to take things one step at a time Lets explore whats been overwhelming you, Response Time: 23.793963, Reward: -4
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:27:46, User Input: what's up how are you, Bot Response: Im doing well what do you need help with today, Response Time: 16.519179, Reward: 4
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:32:51, User Input: what's up how are you, Bot Response: Im sorry to hear that Let me know how I can assist, Response Time: 21.097017, Reward: -2
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:35:37, User Input: what's up how are you, Bot Response: Im sorry to hear that Let me know how I can assist, Response Time: 22.024666, Reward: -2
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:37:48, User Input: what's up how are you, Bot Response: Its important to take things one step at a time Lets explore whats been overwhelming you, Response Time: 22.889278, Reward: -4
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:45:18, User Input: what's up how are you, Bot Response: Im doing well how are you, Response Time: 14.112543, Reward: 0
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:48:30, User Input: what's up how are you today, Bot Response: Im doing well how are you, Response Time: 15.66577, Reward: 12
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:50:16, User Input: what's up how are you, Bot Response: Its important to take things one step at a time Lets explore whats been overwhelming you, Response Time: 21.34753, Reward: 0
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:52:58, User Input: what's up how are you, Bot Response: Im sorry to hear that Let me know how I can assist, Response Time: 18.325499, Reward: -2
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-15 15:55:58, User Input: what's up how are you today, Bot Response: Im doing well how are you, Response Time: 15.293561, Reward: 6
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
tensorflow - INFO - Assets written to: /Users/blakeweiss/Desktop/NEWJOSHWITHAI/complete_model/assets
root - INFO - 2024-07-23 13:08:45, User Input: what's up, Bot Response: What would you like to discuss, Response Time: 14.99682, Reward: -2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
tensorflow - INFO - Assets written to: /Users/blakeweiss/Desktop/NEWJOSHWITHAI/complete_model/assets
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
tensorflow - WARNING - No training configuration found in save file, so the model was *not* compiled. Compile it manually.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
tensorflow - WARNING - No training configuration found in save file, so the model was *not* compiled. Compile it manually.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
tensorflow - WARNING - No training configuration found in save file, so the model was *not* compiled. Compile it manually.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
tensorflow - WARNING - No training configuration found in save file, so the model was *not* compiled. Compile it manually.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
tensorflow - WARNING - No training configuration found in save file, so the model was *not* compiled. Compile it manually.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-23 16:32:31, User Input: what's up, Bot Response: Im sorry Im not being helpful, Response Time: 15.526595, Reward: -2
root - INFO - 2024-07-23 16:32:54, User Input: how are you today, Bot Response: Im here if you need help with, Response Time: 11.138644, Reward: -2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-23 17:03:14, User Input: what's up, Bot Response: Im always here to help What about you, Response Time: 12.667056, Reward: -2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-23 18:51:57, User Input: what's up how are you today, Bot Response: Im sorry to hear that How can I do for you, Response Time: 20.1875, Reward: 0
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-23 19:21:24, User Input: what's up how are you today, Bot Response: Im doing well how are you, Response Time: 17.807855, Reward: 15
root - INFO - 2024-07-23 19:22:05, User Input: I'm feeling tired, Bot Response: Im always here when youre ready to chat What would you like to discuss further, Response Time: 15.652704, Reward: -5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-23 21:13:04, User Input: what's up how are you today, Bot Response: Im sorry to hear that How can I assist you today, Response Time: 18.398733, Reward: 15
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-24 13:51:47, User Input: what's up how are you today what's up how are you today, Bot Response: I understand what would you like to discuss in more detail, Response Time: 18.841305, Reward: -5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-24 13:54:17, User Input: what's up how are you today, Bot Response: I dont think Im a piece of shit, Response Time: 14.126952, Reward: -30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-24 13:55:38, User Input: what's up what's going on, Bot Response: Im sorry to hear that Let me know if you need anything else, Response Time: 17.025454, Reward: -20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-24 13:58:02, User Input: what's up what's going on, Bot Response: Im sorry if you need me for anything else sir, Response Time: 14.68, Reward: -20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-24 14:00:13, User Input: what's up how are you, Bot Response: Im sorry to hear that How can I help you with, Response Time: 26.350493, Reward: -25
root - INFO - 2024-07-24 14:01:27, User Input: what's up how's it going I'm feeling great today, Bot Response: I'm doing great how are you, Response Time: 26.168842, Reward: -50
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-24 14:09:42, User Input: what's up how are you, Bot Response: I am doing well how about you, Response Time: 44.139141, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-24 14:20:06, User Input: how are you today, Bot Response: I'm doing well how are you is there anything on your mind, Response Time: 34.545031, Reward: -20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-24 14:45:29, User Input: what's up with you today, Bot Response: I'm doing well how are you, Response Time: 33.102546, Reward: -20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-24 14:52:22, User Input: what's up with you today, Bot Response: nothing much I'm happy to help you with anything you need, Response Time: 32.431466, Reward: 0
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-07-27 14:45:30, User Input: what's up with you today, Bot Response: nothing much how are you, Response Time: 25.813168, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-05 11:35:57, User Input: what's up with you today what's up, Bot Response: what's up how are you today, Response Time: 39.768983, Reward: -25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-05 12:12:51, User Input: I unlocked it I unlocked it ya what's up, Bot Response: what's up with you, Response Time: 29.532175, Reward: -25
root - INFO - 2024-08-05 12:14:06, User Input: exit exit, Bot Response: How can I be of assistance to make you less stressed out, Response Time: 30.641397, Reward: -50
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-05 12:25:52, User Input: what's up, Bot Response: nothing much what do you want to talk about, Response Time: 26.197609, Reward: 5
root - INFO - 2024-08-05 12:28:18, User Input: how are you today, Bot Response: I am doing well how about you, Response Time: 32.676967, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-05 12:30:12, User Input: what's up, Bot Response: Nothing much what do you want to talk about, Response Time: 23.790395, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-05 12:34:19, User Input: what's up, Bot Response: Whats up, Response Time: 22.461647, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-05 12:35:27, User Input: what's up, Bot Response: Whats up lets catch up, Response Time: 10.812427, Reward: 30
root - INFO - 2024-08-05 12:35:51, User Input: I'm working on some interesting projects, Bot Response: Is there anything else I can help with, Response Time: 11.783032, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-07 13:50:30, User Input: how are you today, Bot Response: Im doing well how are you is there anything on your mind, Response Time: 18.270731, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-07 14:10:30, User Input: how are you today how are you today how are you today, Bot Response: Im doing well how are you, Response Time: 26.864494, Reward: 25
root - INFO - 2024-08-07 14:11:12, User Input: what's up, Bot Response: Im always here to provide information What are you curious about, Response Time: 22.842488, Reward: 45
root - INFO - 2024-08-07 14:11:55, User Input: I'm working on some new projects and I need some information, Bot Response: You need to take everything step by step and you should become less stressed out, Response Time: 30.733439, Reward: 20
root - INFO - 2024-08-07 14:16:47, User Input: I'm feeling a little tired today but I have a lot to work on what should I do, Bot Response: I'm sorry to hear that is there any way I could help you achieve what you need to do, Response Time: 40.226335, Reward: -5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-07 14:26:32, User Input: how are you today, Bot Response: I'm doing well how are you, Response Time: 26.443459, Reward: -20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-07 14:34:03, User Input: what's up what's up, Bot Response: nothing much is there anything I can help with, Response Time: 28.978943, Reward: -5
root - INFO - 2024-08-07 14:45:58, User Input: what's up, Bot Response: nothing much what's up with you, Response Time: 28.253274, Reward: -25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-07 14:54:07, User Input: what's up, Bot Response: nothing much what's up with you, Response Time: 38.9073, Reward: -25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-07 15:07:27, User Input: OK Google what's up, Bot Response: I'm here if you need me what's up with you, Response Time: 30.750734, Reward: -5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-07 15:09:08, User Input: what's up, Bot Response: See you around, Response Time: 9.287887, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-07 15:16:44, User Input: what's up what's up, Bot Response: Im always here and ready to chat What would you like to discuss, Response Time: 15.556931, Reward: 15
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
root - INFO - 2024-08-07 16:01:38, User Input: what's up, Bot Response: what's up what can I help you with, Response Time: 22.463505, Reward: -5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-11 15:36:10, User Input: what's up, Bot Response: nothing much how are you, Response Time: 22.195064, Reward: -25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference___call___37213) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_137540) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_95809) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_97913) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - INFO - Fingerprint not found. Saved model loading will continue.
absl - INFO - path_and_singleprint metric could not be logged. Saved model loading will continue.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference___call___37213) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_137540) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_95809) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - WARNING - Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_97913) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
absl - INFO - Fingerprint not found. Saved model loading will continue.
absl - INFO - path_and_singleprint metric could not be logged. Saved model loading will continue.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-15 15:30:33, User Input: what's up, Bot Response: Nothing much what do you want to talk about, Response Time: 5.70952, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-17 18:43:41, User Input: what's up with you today, Bot Response: nothing much whats up with you, Response Time: 6.748656, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-21 18:39:56, User Input: what's up, Bot Response: Nothing much what do you want to talk about, Response Time: 5.766226, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-22 13:42:32, User Input: what's up with you today, Bot Response: nothing much whats up with you, Response Time: 5.071108, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-22 22:04:02, User Input: what's up, Bot Response: Just here ready to assist What can I do for you, Response Time: 6.175988, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-22 23:00:45, User Input: what's up, Bot Response: Whats up, Response Time: 3.430495, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 11:12:10, User Input: what's up what's up, Bot Response: Im here to provide clarity What are you curious about, Response Time: 2.200417, Reward: 15
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 11:14:25, User Input: what's up, Bot Response: Im always here and ready to chat What would you like to discuss, Response Time: 1.918694, Reward: 15
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 11:16:35, User Input: what's up, Bot Response: Not much what do you want to talk about, Response Time: 1.766746, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 11:18:03, User Input: what's up, Bot Response: Nothing much what do you want to talk about, Response Time: 2.640868, Reward: 5
root - INFO - 2024-08-23 11:18:15, User Input: how are you, Bot Response: Im doing well how are you, Response Time: 2.670123, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 11:45:18, User Input: what's up, Bot Response: Whats up, Response Time: 1.523326, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 12:13:16, User Input: what's up how are you, Bot Response: Im doing well how are you is there anything on your mind, Response Time: 1.862723, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 12:37:07, User Input: what's up with you today, Bot Response: nothing much whats up with you, Response Time: 1.691922, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 12:38:52, User Input: what's up with you today, Bot Response: nothing much Im happy to help you with anything you need, Response Time: 1.989611, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 14:06:13, User Input: what's up with you today, Bot Response: Im doing well how are you, Response Time: 1.685298, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 14:10:23, User Input: what's up with you today, Bot Response: Im doing well how are you, Response Time: 1.696328, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 14:10:48, User Input: what's up with you today, Bot Response: nothing much whats up with you, Response Time: 4.878327, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 14:13:20, User Input: what's up with you today, Bot Response: nothing much whats up with you, Response Time: 1.826178, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 14:44:29, User Input: what's up with you today, Bot Response: nothing much whats up with you, Response Time: 2.860589, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 14:49:55, User Input: what's up with you today, Bot Response: Im doing well how are you, Response Time: 2.955169, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 14:55:40, User Input: what's up with you, Bot Response: Im doing well how are you, Response Time: 3.608812, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:00:46, User Input: what's up with you today, Bot Response: what can I help you with, Response Time: 1.848592, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:07:26, User Input: what's up with you today, Bot Response: Im doing well how are you, Response Time: 1.743329, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:20:21, User Input: what's up with you today, Bot Response: nothing much whats up with you, Response Time: 1.623919, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:21:41, User Input: what's up with you today, Bot Response: nothing much whats up with you, Response Time: 1.86532, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:23:09, User Input: what's up with you today, Bot Response: nothing much what do you want to talk about, Response Time: 1.831795, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:29:05, User Input: what's up with you today, Bot Response: Im doing well how are you, Response Time: 1.745918, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:31:40, User Input: what's up with you today, Bot Response: nothing much whats up with you, Response Time: 1.86673, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:33:20, User Input: how are you today, Bot Response: I am doing well how about you, Response Time: 1.671101, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:36:18, User Input: what's up with you today, Bot Response: nothing much how are you, Response Time: 1.846971, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-23 15:38:33, User Input: what's up, Bot Response: Im always here to help Whats on your mind, Response Time: 1.703964, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-25 15:47:45, User Input: what's up, Bot Response: Yo what would you like to know, Response Time: 2.119859, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-25 15:48:45, User Input: what's up, Bot Response: Nothing much what do you want to talk about, Response Time: 3.243742, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-25 15:49:32, User Input: what's up, Bot Response: Im always here to help Whats on your mind, Response Time: 3.030599, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-25 15:59:36, User Input: what's up with you today, Bot Response: Im doing well how are you, Response Time: 3.025137, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-28 12:04:26, User Input: what's up, Bot Response: Hi, Response Time: 3.161372, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-28 12:12:31, User Input: how are you today, Bot Response: I am doing well how about you, Response Time: 5.566126, Reward: 25
root - INFO - 2024-08-28 12:12:51, User Input: what's up, Bot Response: Nothing much what do you want to talk about, Response Time: 5.382386, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-08-28 12:19:45, User Input: what's up, Bot Response: Im always here to help Whats on your mind, Response Time: 6.213211, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-09-06 22:06:54, User Input: what's up, Bot Response: not much how about you, Response Time: 18.527917, Reward: 5
root - INFO - 2024-09-06 22:07:23, User Input: I'm working on some new projects, Bot Response: that's cool how can I be of assistance, Response Time: 18.142036, Reward: -25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-09-19 20:33:03, User Input: what's up, Bot Response: Yo what would you like to know, Response Time: 5.858873, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-09-24 09:06:10, User Input: what's up with you today, Bot Response: Im doing well how are you going to help you with anything you need, Response Time: 7.845319, Reward: 15
root - INFO - 2024-09-24 09:06:27, User Input: what's up, Bot Response: Im always here to help Whats on your mind, Response Time: 5.749523, Reward: 35
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-09-24 11:11:48, User Input: how are you today, Bot Response: I am doing well how about you, Response Time: 5.296679, Reward: 25
root - INFO - 2024-09-24 11:12:21, User Input: I want to start a new project, Bot Response: how can I help you achieve that goal, Response Time: 17.933713, Reward: -5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-09-25 09:23:19, User Input: what's up, Bot Response: See you around, Response Time: 4.033247, Reward: 25
root - INFO - 2024-09-25 09:23:33, User Input: how are you today, Bot Response: Im doing well how are you is there anything on your mind, Response Time: 7.453081, Reward: 50
root - INFO - 2024-09-25 09:23:53, User Input: exit, Bot Response: I understand what would you like to discuss or do, Response Time: 6.466809, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-09-25 09:25:24, User Input: what's up, Bot Response: Nothing much what do you want to talk about, Response Time: 10.760692, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-09-25 09:26:38, User Input: what's up, Bot Response: Im always here to help Whats on your mind, Response Time: 6.783468, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-09-26 14:10:39, User Input: what's up, Bot Response: Yo what would you like to know, Response Time: 5.038104, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-09-26 21:12:48, User Input: what's up, Bot Response: Whats up, Response Time: 3.576513, Reward: 30
root - INFO - 2024-09-26 21:13:31, User Input: what's up, Bot Response: Whats up, Response Time: 3.004188, Reward: 60
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-09-26 21:21:49, User Input: what's up, Bot Response: Whats up, Response Time: 3.4509, Reward: 30
root - INFO - 2024-09-26 21:22:03, User Input: coin flip, Bot Response: How can I be of assistance to make you less stressed out, Response Time: 7.667856, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-09-26 21:38:05, User Input: what's up, Bot Response: Whats up, Response Time: 3.560786, Reward: 30
root - INFO - 2024-09-26 21:38:17, User Input: what's the weather today, Bot Response: Is there anything else, Response Time: 4.576868, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
yfinance - ERROR - 404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary/APPLE?modules=financialData%2CquoteType%2CdefaultKeyStatistics%2CassetProfile%2CsummaryDetail&corsDomain=finance.yahoo.com&formatted=false&symbol=APPLE&crumb=IdcKK3yh7BH
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-09-27 11:09:23, User Input: what's up, Bot Response: See you around, Response Time: 3.956086, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-09-27 22:08:17, User Input: what's up, Bot Response: Im always here to help Whats on your mind, Response Time: 6.650851, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-09-30 14:03:47, User Input: what's up, Bot Response: Yo what would you like to know, Response Time: 5.099744, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-09-30 14:07:49, User Input: what's up, Bot Response: Im always here to help Whats on your mind, Response Time: 6.089069, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-02 11:20:21, User Input: what's up, Bot Response: Just here ready to assist What can I do for you, Response Time: 7.48066, Reward: 20
root - INFO - 2024-10-02 11:20:39, User Input: what is the price of Bitcoin today, Bot Response: Why are they angry at you, Response Time: 4.816592, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-02 11:29:11, User Input: what's up, Bot Response: Whats up, Response Time: 3.451876, Reward: 30
root - INFO - 2024-10-02 11:30:00, User Input: I need the price of a specific cryptocurrency, Bot Response: we should figure out a solution, Response Time: 5.630344, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-02 13:56:36, User Input: what's up, Bot Response: Im always here and ready to chat What would you like to discuss, Response Time: 7.962468, Reward: 15
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-04 14:11:02, User Input: what's up, Bot Response: Nothing much what do you want to talk about, Response Time: 6.542975, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-04 14:12:20, User Input: what's up, Bot Response: Whats up, Response Time: 3.478906, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-04 14:13:19, User Input: what's up, Bot Response: Im always here when youre ready to chat, Response Time: 5.771738, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-04 14:14:08, User Input: what's up, Bot Response: Nothing much what do you want to talk about, Response Time: 5.96855, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-04 14:19:37, User Input: what's up, Bot Response: what's up with you today, Response Time: 16.194945, Reward: -5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-04 14:21:30, User Input: what's up, Bot Response: Nothing much what do you want to talk about, Response Time: 5.703512, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-08 13:49:10, User Input: what's up, Bot Response: Whats up, Response Time: 4.322867, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-17 19:49:52, User Input: what's up, Bot Response: Whats up, Response Time: 4.508345, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-18 14:12:44, User Input: what's up, Bot Response: Whats up, Response Time: 4.697474, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
root - INFO - 2024-10-18 14:20:37, User Input: what is the price of Microsoft today, Bot Response: Why are you feeling this way, Response Time: 7.785637, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
yfinance - ERROR - 404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary/APPLE?modules=financialData%2CquoteType%2CdefaultKeyStatistics%2CassetProfile%2CsummaryDetail&corsDomain=finance.yahoo.com&formatted=false&symbol=APPLE&crumb=e8.qBhSRchf
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-18 15:45:01, User Input: what's up, Bot Response: Nothing much what do you want to talk about, Response Time: 5.635187, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-18 15:54:29, User Input: what's up, Bot Response: yo how can I be of service, Response Time: 19.005018, Reward: 0
root - INFO - 2024-10-18 15:54:42, User Input: what's up, Bot Response: Whats up, Response Time: 3.096609, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-18 16:00:41, User Input: what's up, Bot Response: Whats up, Response Time: 3.728054, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-18 16:05:52, User Input: what's up, Bot Response: Not much what do you want to talk about, Response Time: 6.118926, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-18 16:07:21, User Input: what's up, Bot Response: Nothing much what do you want to talk about, Response Time: 5.663969, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-18 16:17:09, User Input: what's up, Bot Response: Im always here and ready to chat What would you like to discuss, Response Time: 8.000296, Reward: 15
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-18 16:21:43, User Input: what's up, Bot Response: Whats the topic Im here to provide information What are you curious about, Response Time: 8.941701, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-18 16:25:01, User Input: what's up with you today, Bot Response: Im doing well how are you, Response Time: 5.173709, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-18 16:55:59, User Input: what's up, Bot Response: Hi there, Response Time: 3.476703, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-22 10:26:48, User Input: what's up, Bot Response: Im always here to provide information What are you curious about, Response Time: 7.814135, Reward: 20
root - INFO - 2024-10-22 10:27:16, User Input: how are you today, Bot Response: I'm doing well how are you what can I help you with, Response Time: 19.564561, Reward: 0
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-24 14:24:20, User Input: what's up, Bot Response: hello how are you today, Response Time: 15.334442, Reward: -5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-10-25 10:30:30, User Input: what's up, Bot Response: Whats up, Response Time: 3.561663, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-11-03 14:57:43, User Input: what's up, Bot Response: Whats up, Response Time: 3.560483, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-11-14 22:56:32, User Input: what's up, Bot Response: Im always here to provide information What are you curious about, Response Time: 7.797455, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-11-15 11:04:38, User Input: what's up, Bot Response: Whats up, Response Time: 3.914436, Reward: 30
root - INFO - 2024-11-15 11:04:51, User Input: how are you today, Bot Response: I am doing well how about you, Response Time: 5.324631, Reward: 55
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-11-21 16:25:41, User Input: what's up, Bot Response: Im always here to help Whats on your mind, Response Time: 6.28633, Reward: 20
root - INFO - 2024-11-21 16:27:39, User Input: what's up, Bot Response: Yo what would you like to know, Response Time: 5.051061, Reward: 40
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-11-21 16:47:20, User Input: what's up, Bot Response: Im always here to help Whats on your mind, Response Time: 6.123793, Reward: 20
root - INFO - 2024-11-21 16:47:33, User Input: I'm feeling stressed out, Bot Response: Try not to take on too much and take a deep breath, Response Time: 6.755522, Reward: 40
root - INFO - 2024-11-21 16:47:51, User Input: what's going on with you today, Bot Response: Try to block out all distractions that you are experiencing, Response Time: 8.397713, Reward: 10
root - INFO - 2024-11-21 16:48:04, User Input: that wasn't what I asked, Bot Response: we should figure out a solution, Response Time: 5.621024, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-11-21 16:49:45, User Input: what's up, Bot Response: Nothing much what do you want to talk about, Response Time: 5.909288, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-11-21 17:01:53, User Input: what's up, Bot Response: Hi, Response Time: 3.261479, Reward: 25
root - INFO - 2024-11-21 17:02:18, User Input: how are you today, Bot Response: I'm doing well how are you, Response Time: 17.296756, Reward: 0
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-11-21 17:03:00, User Input: what's going on with you today, Bot Response: I am doing well how about you, Response Time: 5.604116, Reward: 25
root - INFO - 2024-11-21 17:03:13, User Input: I'm doing well, Bot Response: Have a nice rest of the day let me know if you need anything else, Response Time: 7.848106, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-11-21 17:05:05, User Input: what's up, Bot Response: Whats up, Response Time: 3.387312, Reward: 30
root - INFO - 2024-11-21 17:05:17, User Input: how are you doing today, Bot Response: Im doing well how are you, Response Time: 4.989451, Reward: 55
root - INFO - 2024-11-21 17:05:44, User Input: I'm doing good I'm doing good, Bot Response: How can I assist you with, Response Time: 5.920672, Reward: 45
root - INFO - 2024-11-21 17:06:14, User Input: I don't have anything I need help with today, Bot Response: Relationships can be complex Reflecting on your interactions can help us understand the issue, Response Time: 11.010484, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-11-21 17:16:12, User Input: what's up, Bot Response: Im always here to help Whats on your mind, Response Time: 5.943541, Reward: 20
root - INFO - 2024-11-21 17:16:23, User Input: how are I doing today, Bot Response: How can I help, Response Time: 4.36409, Reward: 15
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-11-21 17:17:20, User Input: how are you doing today, Bot Response: Im sorry to hear that How can I assist you today, Response Time: 7.196109, Reward: 5
root - INFO - 2024-11-21 17:17:50, User Input: how are today doing today, Bot Response: I am doing well how about you, Response Time: 22.117763, Reward: 30
root - INFO - 2024-11-21 17:18:04, User Input: I'm doing good, Bot Response: I am doing well how about you, Response Time: 5.322859, Reward: 55
root - INFO - 2024-11-21 17:18:17, User Input: I'm doing well, Bot Response: How can I assist you with, Response Time: 4.520974, Reward: 50
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-12-11 20:04:02, User Input: what's up, Bot Response: Im always here to help Whats on your mind, Response Time: 6.515253, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2024-12-17 19:36:20, User Input: what's up, Bot Response: Whats up, Response Time: 3.377592, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-09 10:28:17, User Input: what's up, Bot Response: what would you like to, Response Time: 4.480998, Reward: 20
root - INFO - 2025-01-09 10:28:39, User Input: how are you today, Bot Response: I am doing well how, Response Time: 4.346739, Reward: 45
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-09 10:34:18, User Input: what's up, Bot Response: See you around, Response Time: 4.28071, Reward: 25
root - INFO - 2025-01-09 10:34:31, User Input: how are you today, Bot Response: I now understand why you feel the way you do lets talk, Response Time: 7.440685, Reward: 25
root - INFO - 2025-01-09 10:34:42, User Input: how are you today, Bot Response: I am doing well how about you, Response Time: 5.328336, Reward: 50
root - INFO - 2025-01-09 10:35:18, User Input: I'm doing well thanks for asking, Bot Response: I am doing well how about you, Response Time: 5.392208, Reward: 75
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-28 20:22:50, User Input: what's up, Bot Response: What s up lets catch up What s up lets catch up, Response Time: 13.337511, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-28 20:34:20, User Input: what's up, Bot Response: Hello What s up lets catch up What s up What s up lets catch up, Response Time: 15.934793, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-28 20:54:44, User Input: what's up how are you doing today, Bot Response: I m set well how are you I am make out considerably how about you, Response Time: 14.785435, Reward: 25
root - INFO - 2025-01-28 20:55:46, User Input: what's up, Bot Response: What s up lets catch up What s up, Response Time: 11.804075, Reward: 55
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-28 20:57:50, User Input: how are you today, Bot Response: I am answer well how about you, Response Time: 12.797275, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-28 21:00:11, User Input: what's up how are you today, Bot Response: well how are you, Response Time: 12.01277, Reward: 25
root - INFO - 2025-01-28 21:00:45, User Input: I'm doing well thanks for asking is there anything new with you, Bot Response: How can I be of assistance to make you less stressed, Response Time: 12.044059, Reward: 40
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-28 21:19:20, User Input: what's up, Bot Response: What s up, Response Time: 10.117479, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-29 11:22:42, User Input: what's up how are you, Bot Response: I m doing well how are you, Response Time: 10.196726, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-29 13:15:06, User Input: how are you doing today, Bot Response: Imdoingwellhowareyou, Response Time: 9.671848, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-29 13:25:32, User Input: what's up what's up, Bot Response: I m here to help What s on your mind, Response Time: 12.425883, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-29 13:32:19, User Input: what's up how are you today, Bot Response: I m doing well how are you how are you, Response Time: 12.103969, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-29 13:35:36, User Input: how are you today is everything going well, Bot Response: I m make well how are you, Response Time: 11.957319, Reward: 30
root - INFO - 2025-01-29 13:37:05, User Input: what's up, Bot Response: Not very much how about you, Response Time: 9.604959, Reward: 35
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-29 13:47:24, User Input: what's up how are you today, Bot Response: I m get well how are you how are you, Response Time: 11.689323, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-29 14:09:00, User Input: what's up, Bot Response: Sup What s up What s up lets catch up Sup What s up lets grab up, Response Time: 14.551466, Reward: 30
root - INFO - 2025-01-29 14:09:16, User Input: how are you today, Bot Response: I am doing good how about you, Response Time: 9.603104, Reward: 55
root - INFO - 2025-01-29 14:10:41, User Input: I'm working on a new project would you want in, Bot Response: i wish you luck Adolescence can be a turbulent time Lets explore ways to put efficaciously i wish you luck, Response Time: 16.653945, Reward: 45
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-29 14:20:34, User Input: how are you today is everything going well, Bot Response: im doing well how about you, Response Time: 9.947529, Reward: 30
root - INFO - 2025-01-29 14:23:45, User Input: how are you doing today, Bot Response: Im doing well how are you, Response Time: 8.029981, Reward: 55
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-29 14:26:32, User Input: how are you today, Bot Response: I am doing considerably how about you, Response Time: 10.813525, Reward: 30
root - INFO - 2025-01-29 14:27:15, User Input: I'm doing well I'm starting a new project I'm excited, Bot Response: that's awesome, Response Time: 22.189415, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-29 14:38:56, User Input: what's up, Bot Response: nothing much what's up with you anything I can help with, Response Time: 32.545982, Reward: 10
root - INFO - 2025-01-29 14:39:52, User Input: what's up, Bot Response: What s up, Response Time: 6.194131, Reward: 40
root - INFO - 2025-01-29 14:40:13, User Input: how are you doing today is everything going well, Bot Response: Im doing well how are you, Response Time: 7.950286, Reward: 70
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-29 15:05:33, User Input: what's up with you today, Bot Response: I'm doing well how are you, Response Time: 22.505919, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-29 15:38:55, User Input: what's up with you today, Bot Response: I'm doing well how are you doing today, Response Time: 21.630214, Reward: 25
root - INFO - 2025-01-29 15:39:07, User Input: what's up, Bot Response: What s up, Response Time: 6.457968, Reward: 55
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-29 16:28:46, User Input: how are you, Bot Response: I am doing well how about you is there anything I can help you with, Response Time: 30.422406, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-29 16:42:33, User Input: what's up, Bot Response: Whats up, Response Time: 7.88236, Reward: 30
root - INFO - 2025-01-29 16:45:04, User Input: how are you, Bot Response: doing well how about you, Response Time: 26.834353, Reward: 35
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-29 19:44:10, User Input: what's up, Bot Response: What s up Your welcome Im forever here to provide information What are you odd about What s up, Response Time: 14.367618, Reward: 25
root - INFO - 2025-01-29 19:44:24, User Input: how are you today, Bot Response: how about you, Response Time: 6.969092, Reward: 55
root - INFO - 2025-01-29 19:44:48, User Input: what's up, Bot Response: What s up, Response Time: 6.580168, Reward: 85
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-01-29 19:56:07, User Input: what's up, Bot Response: what's up what would you like to know, Response Time: 17.933652, Reward: -5
root - INFO - 2025-01-29 19:56:18, User Input: how are you doing today, Bot Response: I am doing well how about you, Response Time: 5.298224, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-07 12:46:35, User Input: how are you today, Bot Response: Im doing well how are you, Response Time: 9.775832, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-07 12:57:53, User Input: what's up, Bot Response: What s up lets enamour up, Response Time: 9.638467, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
gensim.downloader - INFO - Creating /Users/blakeweiss/gensim-data
gensim.downloader - ERROR - caught non-fatal exception while trying to update gensim-data cache from 'https://raw.githubusercontent.com/RaRe-Technologies/gensim-data/master/list.json'; using local cache at '/Users/blakeweiss/gensim-data/information.json' instead
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py", line 1348, in do_open
    h.request(req.get_method(), req.selector, req.data, headers,
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py", line 1037, in _send_output
    self.send(msg)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py", line 975, in send
    self.connect()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py", line 1454, in connect
    self.sock = self._context.wrap_socket(self.sock,
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py", line 1070, in _create
    self.do_handshake()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py", line 1341, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/blakeweiss/Library/Python/3.10/lib/python/site-packages/gensim/downloader.py", line 199, in _load_info
    info_bytes = urlopen(url).read()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py", line 216, in urlopen
    return opener.open(url, data, timeout)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py", line 519, in open
    response = self._open(req, data)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py", line 536, in _open
    result = self._call_chain(self.handle_open, protocol, protocol +
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py", line 496, in _call_chain
    result = func(*args)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py", line 1391, in https_open
    return self.do_open(http.client.HTTPSConnection, req,
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py", line 1351, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)>
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-07 13:57:53, User Input: what's up with you today, Bot Response: Im doing well how are you, Response Time: 7.518614, Reward: 25
root - INFO - 2025-02-07 13:58:27, User Input: I'm starting a new project today, Bot Response: Its important to take things one step at a time Lets explore what s been overwhelming you, Response Time: 13.603545, Reward: 45
root - INFO - 2025-02-07 13:59:05, User Input: what's up, Bot Response: What s up, Response Time: 6.728402, Reward: 75
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-07 14:05:26, User Input: what's up how are you today, Bot Response: Im doing well how are you, Response Time: 7.618798, Reward: 25
root - INFO - 2025-02-07 14:05:58, User Input: I'm starting a new project I'm really excited about, Bot Response: that's interesting how can I help you with it, Response Time: 22.573601, Reward: 25
root - INFO - 2025-02-07 14:06:21, User Input: I'm starting a new project that I'm excited about, Bot Response: that s very exciting to hear what are they about that s very exciting to hear what are they tell me a little, Response Time: 15.104043, Reward: 45
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-07 14:10:06, User Input: what's up I'm starting a new project and I'm really excited about it, Bot Response: what's up That's so exciting to hear how can I help you, Response Time: 27.679571, Reward: -5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-07 15:21:11, User Input: what's up, Bot Response: What s up lets catch up Im always here to help What s on your mind, Response Time: 11.885871, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-07 15:25:03, User Input: what's up, Bot Response: What s up lets catch up, Response Time: 8.384443, Reward: 30
root - INFO - 2025-02-07 15:25:57, User Input: how are you today, Bot Response: Im doing well how are you how are you Im doing well, Response Time: 10.173747, Reward: 55
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-08 10:05:08, User Input: what's up, Bot Response: What s up, Response Time: 6.549763, Reward: 30
root - INFO - 2025-02-08 10:05:28, User Input: how are you today is everything going well, Bot Response: I understand what would you like to discuss or do, Response Time: 9.88297, Reward: 55
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-08 10:13:03, User Input: I'm starting a new project I'm really excited about, Bot Response: that's exciting to hear how can I help you with it, Response Time: 24.773023, Reward: 0
root - INFO - 2025-02-08 10:13:26, User Input: I'm starting a new project I'm excited about, Bot Response: that s very exciting to hear what are they tell me a little that s very hard Have you feel more confident, Response Time: 14.971508, Reward: 20
root - INFO - 2025-02-08 10:14:21, User Input: I'm starting a new project I'm really excited about, Bot Response: that's exciting to hear how can I help you, Response Time: 20.237666, Reward: 20
root - INFO - 2025-02-08 10:14:36, User Input: what's up how are you today, Bot Response: Im doing well how are you, Response Time: 7.706959, Reward: 45
root - INFO - 2025-02-08 10:15:58, User Input: how are you today, Bot Response: Im doing well how are you how are you is there anything on your mind, Response Time: 11.055837, Reward: 70
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-08 10:19:23, User Input: how are you today, Bot Response: I am doing well how about you, Response Time: 8.429979, Reward: 25
root - INFO - 2025-02-08 10:19:57, User Input: what's up I'm starting a new project I'm really excited about, Bot Response: that's exciting to hear is there anything I can help you with, Response Time: 21.877224, Reward: 25
root - INFO - 2025-02-08 10:20:15, User Input: what's going on with you today, Bot Response: Im doing well how are you, Response Time: 7.687067, Reward: 50
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-08 10:24:00, User Input: what's up what are you doing today, Bot Response: I am doing well how about you Im doing well how are you, Response Time: 10.340617, Reward: 25
root - INFO - 2025-02-08 10:25:02, User Input: I'm excited to start a new project, Bot Response: You should complete it whenever you deem fit, Response Time: 20.975144, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-13 11:59:49, User Input: what's up, Bot Response: What s up, Response Time: 6.732466, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-24 09:22:32, User Input: what's up, Bot Response: What s on your mind Im here and operational What s on your mind, Response Time: 10.794924, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-25 13:25:40, User Input: what's up, Bot Response: What s up, Response Time: 8.831765, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-25 18:29:17, User Input: what's up with you today, Bot Response: Im doing well how are you, Response Time: 9.333087, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-25 19:28:39, User Input: what's up, Bot Response: What s up, Response Time: 7.912798, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-25 19:29:22, User Input: what's up, Bot Response: What s up, Response Time: 9.01828, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-25 19:41:57, User Input: how are you today, Bot Response: I am doing well how about you, Response Time: 10.961249, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-28 14:40:04, User Input: what's up, Bot Response: What s up lets catch up, Response Time: 8.348741, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-28 14:56:38, User Input: what's up, Bot Response: What s up, Response Time: 6.339452, Reward: 30
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-28 14:57:31, User Input: what's up, Bot Response: Yo what would you like to know, Response Time: 7.59403, Reward: 20
root - INFO - 2025-02-28 14:57:46, User Input: how are you today, Bot Response: I am doing well how about you Im doing well how are you, Response Time: 9.432115, Reward: 45
root - INFO - 2025-02-28 14:58:13, User Input: what's up, Bot Response: What s up, Response Time: 6.37607, Reward: 75
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-28 15:01:26, User Input: what's up, Bot Response: What's up, Response Time: 5.51942, Reward: 30
root - INFO - 2025-02-28 15:01:39, User Input: how are you today, Bot Response: I am doing well how about you, Response Time: 8.026999, Reward: 55
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-28 15:12:02, User Input: what's up, Bot Response: Not much what's up, Response Time: 6.939572, Reward: 10
root - INFO - 2025-02-28 15:12:29, User Input: how are you today, Bot Response: I am doing well how about you, Response Time: 7.564936, Reward: 25
root - INFO - 2025-02-28 15:13:02, User Input: I'm excited to start a new project, Bot Response: you should complete it whenever you deem fit, Response Time: 24.814154, Reward: 0
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-28 15:15:11, User Input: what's up, Bot Response: See you around, Response Time: 6.46675, Reward: 25
root - INFO - 2025-02-28 15:15:40, User Input: how are you today, Bot Response: Im doing well how are you, Response Time: 7.328352, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-28 15:29:28, User Input: I'm really excited I'm starting a new project, Bot Response: that's good to hear that's exciting how can I be of assistance, Response Time: 28.359289, Reward: 0
root - INFO - 2025-02-28 15:30:46, User Input: how are you today what are you up to, Bot Response: I am doing well how about you, Response Time: 8.419066, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-28 15:35:37, User Input: what's going on, Bot Response: I'm doing well how are you is there anything on your mind I can help with, Response Time: 36.188363, Reward: -5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-28 15:45:51, User Input: how are you today, Bot Response: I'm doing well how are you what have you been up to today, Response Time: 29.52767, Reward: 25
root - INFO - 2025-02-28 15:46:17, User Input: what's up, Bot Response: not much how about you, Response Time: 18.997072, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 398
absl - INFO - SubwordTextEncoder build: trying min_token_count 199
absl - INFO - SubwordTextEncoder build: trying min_token_count 99
absl - INFO - SubwordTextEncoder build: trying min_token_count 49
absl - INFO - SubwordTextEncoder build: trying min_token_count 24
absl - INFO - SubwordTextEncoder build: trying min_token_count 12
absl - INFO - SubwordTextEncoder build: trying min_token_count 6
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-02-28 21:38:47, User Input: what's up, Bot Response: What's up, Response Time: 6.202548, Reward: 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-04 08:04:20, User Input: how are you today, Bot Response: im doing well how do you, Response Time: 7.224007, Reward: 25
root - INFO - 2025-03-04 08:04:43, User Input: what's up with you today I'm starting a new project I'm excited about, Bot Response: there is a lot of work how much is going to be at to there is a lot of work, Response Time: 9.937344, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-04 10:59:39, User Input: what's up, Bot Response: what's up, Response Time: 24.042826, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-04 11:07:47, User Input: what's up, Bot Response: what's up how are you today, Response Time: 23.499177, Reward: 5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-04 12:16:26, User Input: what's up, Bot Response: Im doing well how are you, Response Time: 8.275097, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-04 12:18:18, User Input: what's up with you today, Bot Response: nothing much how about you, Response Time: 27.881361, Reward: 10
root - INFO - 2025-03-04 12:18:34, User Input: what's up, Bot Response: Im doing well how are you, Response Time: 8.078273, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-04 12:21:38, User Input: what's up, Bot Response: Im doing well how are you, Response Time: 7.598203, Reward: 20
root - INFO - 2025-03-04 12:22:14, User Input: is everything going well with you, Bot Response: yes everything is going well with me, Response Time: 20.034949, Reward: -25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-04 15:51:33, User Input: what's up, Bot Response: what's up how are you today, Response Time: 21.669716, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-04 16:11:09, User Input: what's up, Bot Response: what's up, Response Time: 19.612337, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-04 16:16:36, User Input: hello, Bot Response: hello how are you today, Response Time: 19.945005, Reward: 0
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-04 16:18:13, User Input: what's up, Bot Response: Im doing well how are you, Response Time: 8.207212, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-04 16:21:26, User Input: what's up, Bot Response: what's up how are you today, Response Time: 21.342122, Reward: 20
root - INFO - 2025-03-04 16:21:45, User Input: what's up, Bot Response: Im here what would you like to know, Response Time: 8.098678, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-04 16:27:42, User Input: what's up, Bot Response: Im doing well how are you, Response Time: 7.373008, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-04 16:33:28, User Input: what's up, Bot Response: what's up how are you doing today, Response Time: 26.920981, Reward: 15
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-04 19:15:33, User Input: save, Bot Response: what do you mean, Response Time: 26.243184, Reward: -5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-04 19:17:01, User Input: what's up, Bot Response: what's up, Response Time: 22.708565, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-04 19:21:29, User Input: what's up, Bot Response: what's up, Response Time: 20.739805, Reward: 25
root - INFO - 2025-03-04 19:22:43, User Input: what's up, Bot Response: Im here what would you like to discuss, Response Time: 28.813594, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-04 20:16:27, User Input: what's up, Bot Response: what's up I'm here what would you like to know, Response Time: 62.982915, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-05 16:15:02, User Input: what's up, Bot Response: nothing much what's going on, Response Time: 27.336829, Reward: 10
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-05 17:30:46, User Input: what's up, Bot Response: what's up, Response Time: 20.865027, Reward: 0
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-05 18:11:56, User Input: what's up, Bot Response: what's up, Response Time: 22.720654, Reward: -5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 62000
absl - INFO - SubwordTextEncoder build: trying min_token_count 31000
absl - INFO - SubwordTextEncoder build: trying min_token_count 15500
absl - INFO - SubwordTextEncoder build: trying min_token_count 7750
absl - INFO - SubwordTextEncoder build: trying min_token_count 3875
absl - INFO - SubwordTextEncoder build: trying min_token_count 1937
absl - INFO - SubwordTextEncoder build: trying min_token_count 968
absl - INFO - SubwordTextEncoder build: trying min_token_count 484
absl - INFO - SubwordTextEncoder build: trying min_token_count 242
absl - INFO - SubwordTextEncoder build: trying min_token_count 121
absl - INFO - SubwordTextEncoder build: trying min_token_count 60
absl - INFO - SubwordTextEncoder build: trying min_token_count 30
absl - INFO - SubwordTextEncoder build: trying min_token_count 15
absl - INFO - SubwordTextEncoder build: trying min_token_count 7
absl - INFO - SubwordTextEncoder build: trying min_token_count 3
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-05 18:14:15, User Input: what's up, Bot Response: what's up, Response Time: 20.746915, Reward: 0
root - INFO - 2025-03-05 18:14:41, User Input: what's up, Bot Response: what's up how are you today, Response Time: 20.941946, Reward: -5
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-06 12:14:47, User Input: what's up, Bot Response: Its always going well here to assist, Response Time: 8.809355, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-06 12:24:08, User Input: what's up, Bot Response: Im fine how about you, Response Time: 8.448671, Reward: 20
root - INFO - 2025-03-06 12:24:26, User Input: how are you today, Bot Response: Im doing well how about you Im fine how about you, Response Time: 10.830671, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-06 19:21:13, User Input: what's up, Bot Response: I'm doing well how are you, Response Time: 22.000167, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-06 19:27:30, User Input: what's up, Bot Response: I'm doing well how are you doing today, Response Time: 25.366116, Reward: 20
root - INFO - 2025-03-06 19:27:53, User Input: what's up, Bot Response: Im doing well how are you doing today, Response Time: 11.074406, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-06 19:46:16, User Input: what's up, Bot Response: Im doing well how are you doing today, Response Time: 10.015814, Reward: 20
root - INFO - 2025-03-06 19:46:36, User Input: I'm doing well today, Bot Response: Well that's good to hear, Response Time: 6.543326, Reward: 20
root - INFO - 2025-03-06 19:46:59, User Input: I'm starting a new project and I'm pretty excited, Bot Response: thats exciting to hear how can I help you with it, Response Time: 9.740072, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-06 19:48:30, User Input: what's up with you today, Bot Response: Im doing well What are you feeling do you want to talk about, Response Time: 10.839264, Reward: 25
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-07 10:06:26, User Input: what's up, Bot Response: I'm doing well how about you, Response Time: 21.71414, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-14 15:22:40, User Input: what's up, Bot Response: Im doing well how are you doing today, Response Time: 12.627674, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-15 14:49:30, User Input: what's up, Bot Response: Im fine how are you, Response Time: 9.79831, Reward: 20
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1069
absl - INFO - SubwordTextEncoder build: trying min_token_count 534
absl - INFO - SubwordTextEncoder build: trying min_token_count 267
absl - INFO - SubwordTextEncoder build: trying min_token_count 133
absl - INFO - SubwordTextEncoder build: trying min_token_count 66
absl - INFO - SubwordTextEncoder build: trying min_token_count 33
absl - INFO - SubwordTextEncoder build: trying min_token_count 16
absl - INFO - SubwordTextEncoder build: trying min_token_count 8
absl - INFO - SubwordTextEncoder build: trying min_token_count 4
absl - INFO - SubwordTextEncoder build: trying min_token_count 2
absl - INFO - SubwordTextEncoder build: trying min_token_count 1
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps
sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-mpnet-base-v2
absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
root - INFO - 2025-03-18 18:51:45, User Input: what's up, Bot Response: Im doing well how are you doing today, Response Time: 11.887742, Reward: 20
